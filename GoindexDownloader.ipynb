{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GoindexDownloader.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rameskr/RcloneLab/blob/master/GoindexDownloader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bCnUMUg_SoT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Px-KV1aN9X98",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!pip install selenium\n",
        "!pip install bs4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7opUlUe19fqG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import sys\n",
        "import requests\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import OrderedDict \n",
        "chromedriver = \"/usr/bin/chromedriver\"\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "d = webdriver.Chrome(chromedriver,options=chrome_options)\n",
        "destinationDir = \"/content/drive/Shared drives/PankajCCSF/test\"\n",
        "url = 'https://edu.tuts.workers.dev/Udacity%20-%20Collections%20[300%20GB]/Nanodegrees/Business%20Analytics%20Nanodegree%20v2.0.0/'\n",
        "d.get(url)\n",
        "time.sleep(7)\n",
        "page_html = d.page_source\n",
        "\n",
        "#crawler starts here\n",
        "def crawler(page_html):\n",
        "    folderDic = {}\n",
        "    fileDic = {}\n",
        "    soup = BeautifulSoup(page_html, 'html.parser')\n",
        "    for fileFolders in soup.select('li.mdui-list-item a'):\n",
        "        if (fileFolders.contents[1].text.find(\"folder_open\") > -1):\n",
        "            tempFolderName = fileFolders.contents[1].text\n",
        "            folderName = tempFolderName.replace(\"folder_open\", \"\").strip().strip().replace(\"/\",\"\")\n",
        "            folderDic[folderName] = \"https://edu.tuts.workers.dev\" + fileFolders['href']\n",
        "        else:\n",
        "            tempFileName = fileFolders.contents[1].text\n",
        "            fileName = tempFileName.replace(\"insert_drive_file\", \"\").strip().replace(\"/\",\"\")\n",
        "            fileDic[fileName] = \"https://edu.tuts.workers.dev\" + fileFolders['href'].replace(\"?a=view\", \"\")\n",
        "\n",
        "    return folderDic,fileDic\n",
        "    #crawler ends here\n",
        "\n",
        "folderDictonary,fileDictonary = crawler(page_html)\n",
        "for folderName, folderUrl in folderDictonary.items():\n",
        "  saveDir = os.path.join(destinationDir,folderName)\n",
        "  if(os.path.exists(saveDir)):\n",
        "    print(\"skipping => \" + saveDir)\n",
        "    continue\n",
        "  print(\"Downloading =>\" + saveDir)  \n",
        "  try:\n",
        "    os.mkdir(saveDir)\n",
        "  except Exception as e:\n",
        "    pass  \n",
        "  d.get(folderUrl)\n",
        "  time.sleep(9)\n",
        "  lvlTwoSource = d.page_source\n",
        "  folderDictonaryLvlTwo,fileDictonaryLvlTwo = crawler(lvlTwoSource)\n",
        "  if(len(folderDictonaryLvlTwo)==0 and len(fileDictonaryLvlTwo)==0):\n",
        "    print(saveDir)\n",
        "    print(\"Content might not have been downloaded for above folder . Please consider increasing sleep time on line number 50 and delete the empty folder\")\n",
        "  # sys.exit()\n",
        "  for folderNameTwo, folderUrlTwo in folderDictonaryLvlTwo.items(): \n",
        "    lvlTwoSaveDir = os.path.join(saveDir,folderNameTwo)\n",
        "    try:\n",
        "      os.mkdir(lvlTwoSaveDir)\n",
        "    except Exception as e:\n",
        "      pass\n",
        "\n",
        "  #actual file processing\n",
        "\n",
        "  for fileNameTwo, fileUrlTwo in fileDictonaryLvlTwo.items(): \n",
        "    if(os.path.exists(os.path.join(saveDir,fileNameTwo))):\n",
        "      print(\"skipping => \" + os.path.join(saveDir,fileNameTwo))\n",
        "      continue \n",
        "    r = requests.get(fileUrlTwo,stream=True)\n",
        "    with open(os.path.join(saveDir,fileNameTwo),\"ab+\") as f:\n",
        "      f.write(r.content)\n",
        "\n",
        "  # print(folderName + \"=>\" + folderUrl)\n",
        "\n",
        "for fileNameMainDir, fileUrlMain in fileDictonary.items():\n",
        "  # if(os.path.exists(os.path.join(destinationDir,fileNameMainDir))):\n",
        "  #   print(\"skipping =>\" + os.path.join(destinationDir,fileNameMainDir))  \n",
        "  #   continue\n",
        "  r = requests.get(fileUrlMain,stream=True)\n",
        "  with open(os.path.join(destinationDir,fileNameMainDir),\"ab+\") as f:\n",
        "    print(\"Downloading =>\" + os.path.join(destinationDir,fileNameMainDir))\n",
        "    f.write(r.content)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}